{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义原始效率计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage: 58348 MB / 196608 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchprofile\n",
    "import subprocess\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "def get_gpu_memory():\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'\n",
    "        ]).decode('utf-8').strip().split('\\n')\n",
    "    \n",
    "    used_memory = 0\n",
    "    total_memory = 0\n",
    "    for res in result:\n",
    "        used, total = map(int, res.split(','))\n",
    "        used_memory += used\n",
    "        total_memory += total\n",
    "\n",
    "    return used_memory, total_memory\n",
    "\n",
    "# 测试函数\n",
    "used, total = get_gpu_memory()\n",
    "print(f\"GPU Memory Usage: {used} MB / {total} MB\")\n",
    "\n",
    "def evaluate_model_efficiency(model, input, criterion, optimizer):\n",
    "    device = torch.device(\"cuda:5\")\n",
    "\n",
    "    padding_mask = torch.zeros_like(input)\n",
    "    x_mark_enc = padding_mask.float().to(device)\n",
    "    x_enc = input.to(device)\n",
    "    model = model.to(device)\n",
    "    # model = DataParallel(model)\n",
    "    # model.to('cuda')\n",
    "    # 将所有的输入数据打包成一个列表或元组\n",
    "    inputs = [x_enc, x_mark_enc]\n",
    "\n",
    "    # 将所有的非张量参数打包成一个字典\n",
    "    kwargs = { 'mask':None,'enable_mask':None,'task_id': 1, 'task_name': 'classification'}\n",
    "\n",
    "    # 使用*运算符将输入列表解包，使用**运算符将关键字参数字典解包\n",
    "    # 评估前向传播时间\n",
    "    start_time = time.time()\n",
    "    output = model(*inputs, **kwargs)\n",
    "    end_time = time.time()\n",
    "    forward_time = end_time - start_time\n",
    "    \n",
    "    # 评估前向和反向传播时间\n",
    "    start_time = time.time()\n",
    "    output = model(*inputs, **kwargs)\n",
    "    target = torch.randn_like(output).to(device)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    inputs_tuple = (x_enc, x_mark_enc,None,None,None,1,'classification',None)\n",
    "\n",
    "    # 获取 FLOPs\n",
    "    flops = torchprofile.profile_macs(model, inputs_tuple) / 1e9\n",
    "    \n",
    "    # # 获取 GPU 显存使用情况\n",
    "    used_memory, total_memory = get_gpu_memory()\n",
    "    used_memory, total_memory = used_memory/1024 , total_memory/1024\n",
    "    print(\"Forward pass time: {:.6f} seconds\".format(forward_time))\n",
    "    print(\"Forward and backward pass time: {:.6f} seconds\".format(total_time))\n",
    "    print(\"FLOPs: {:.6f} GFLOPs\".format(flops))\n",
    "    print(f\"GPU Memory Usage: {used_memory:.2f} GB / {total_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL_task\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from exp.exp_sup import Exp_All_Task as Exp_All_Task_SUP\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class TrainingConfig:\n",
    "    def __init__(Exp):\n",
    "        Exp.parser = argparse.ArgumentParser(description='RmGPT supervised training')\n",
    "        Exp.add_arguments()\n",
    "\n",
    "    def add_arguments(Exp):\n",
    "        # basic config\n",
    "        Exp.parser.add_argument('--task_name', type=str, default='ALL_task', help='task name')\n",
    "        Exp.parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "        Exp.parser.add_argument('--model_id', type=str, default='test', help='model id')\n",
    "        Exp.parser.add_argument('--model', type=str, default='RmGPT', help='model name')\n",
    "\n",
    "        # data loader\n",
    "        Exp.parser.add_argument('--data', type=str, default='All', help='dataset type')\n",
    "        Exp.parser.add_argument('--features', type=str, default='M',\n",
    "                                 help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "        Exp.parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "        Exp.parser.add_argument('--freq', type=str, default='h',\n",
    "                                 help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "        Exp.parser.add_argument('--task_data_config_path', type=str, default='data_provider/data_config/main_result/multi_task_small.yaml', help='root path of the task and data yaml file')\n",
    "        Exp.parser.add_argument('--subsample_pct', type=str, default=None, help='subsample percent')\n",
    "        \n",
    "        # device settings\n",
    "        Exp.parser.add_argument('--device', type=str, default='cuda:0', help='device')\n",
    "        \n",
    "        # ddp settings\n",
    "        Exp.parser.add_argument('--ddp', type=bool, default=False, help='whether to use ddp')\n",
    "        Exp.parser.add_argument('--local-rank', type=int, help='local rank')\n",
    "        Exp.parser.add_argument(\"--dist_url\", default=\"env://\", type=str, help='url used to set up distributed training')\n",
    "        Exp.parser.add_argument('--num_workers', type=int, default=8, help='data loader num workers')\n",
    "        Exp.parser.add_argument(\"--memory_check\", action=\"store_true\", default=False)\n",
    "        Exp.parser.add_argument(\"--large_model\", action=\"store_true\", default=True)\n",
    "\n",
    "        # optimization settings\n",
    "        Exp.parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "        Exp.parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "        Exp.parser.add_argument(\"--prompt_tune_epoch\", type=int, default=0)\n",
    "        Exp.parser.add_argument('--warmup_epochs', type=int, default=0, help='warmup epochs')\n",
    "        Exp.parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "        Exp.parser.add_argument('--acc_it', type=int, default=1, help='acc iteration to enlarge batch size')\n",
    "        Exp.parser.add_argument('--learning_rate', type=float, default=0.0003, help='optimizer learning rate')\n",
    "        Exp.parser.add_argument('--min_lr', type=float, default=None, help='optimizer min learning rate')\n",
    "        Exp.parser.add_argument('--weight_decay', type=float, default=0.0, help='optimizer weight decay')\n",
    "        Exp.parser.add_argument('--layer_decay', type=float, default=None, help='optimizer layer decay')\n",
    "        Exp.parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "        Exp.parser.add_argument('--lradj', type=str, default='prompt_tuning', help='adjust learning rate')\n",
    "        Exp.parser.add_argument('--clip_grad', type=float, default=5.0, help='Clip gradient norm')\n",
    "        Exp.parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "        Exp.parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='save location of model checkpoints')\n",
    "        Exp.parser.add_argument('--pretrained_weight', type=str, default='None')\n",
    "        Exp.parser.add_argument('--debug', type=str, default='disabled', help='debug mode')\n",
    "        Exp.parser.add_argument('--project_name', type=str, default='RmGPT-multitask', help='wandb project name')\n",
    "\n",
    "        # model settings\n",
    "        Exp.parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "        Exp.parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "        Exp.parser.add_argument('--e_layers', type=int, default=4, help='num of encoder layers')\n",
    "        Exp.parser.add_argument(\"--share_embedding\", action=\"store_true\", default=False)\n",
    "        Exp.parser.add_argument(\"--patch_len\", type=int, default=256)\n",
    "        Exp.parser.add_argument(\"--stride\", type=int, default=256)\n",
    "        Exp.parser.add_argument(\"--prompt_num\", type=int, default=10)\n",
    "        Exp.parser.add_argument('--fix_seed', type=int, default=2024, help='seed')\n",
    "        Exp.parser.add_argument(\"--input_len\", type=int, default=2048)\n",
    "        Exp.parser.add_argument('--mode_debug',type=bool,default=False,help='whether to debug')\n",
    "\n",
    "        # task related settings\n",
    "        Exp.parser.add_argument('--inverse', action='store_true', default=False, help='inverse output data')\n",
    "        Exp.parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "        Exp.parser.add_argument('--anomaly_ratio', type=float, default=1.0, help='prior anomaly ratio (%)')\n",
    "        Exp.parser.add_argument(\"--offset\", type=int, default=0)\n",
    "        Exp.parser.add_argument(\"--max_offset\", type=int, default=0)\n",
    "        Exp.parser.add_argument('--zero_shot_forecasting_new_length', type=str, default=None, help='unify')\n",
    "\n",
    "    def parse(Exp, args=None):\n",
    "        # 如果没有提供 args，使用 sys.argv[1:] 的默认行为\n",
    "        # 在 Jupyter 中，可以传递空列表来使用默认值\n",
    "        if args is None:\n",
    "            args = sys.argv[1:]\n",
    "        return Exp.parser.parse_args(args)\n",
    "\n",
    "# Example usage in Jupyter:\n",
    "config = TrainingConfig()\n",
    "args = config.parse([])  # 传递空列表以避免解析命令行参数\n",
    "\n",
    "print(args.task_name)  # 输出一个参数的值以检查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入CWRU数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device id cuda:0\n",
      "Non-cross condition setting when [train stage]\n",
      "[SMU] start_percentage: 0.0, end_percentage: 0.8\n",
      "PHM_SMU 114\n",
      "PHM_Challenge2024 12240\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_All_Task_SUP(args)\n",
    "_,data_loader_list = Exp._get_data(flag='train')\n",
    "data_loader = data_loader_list[1]\n",
    "batch = next(iter(data_loader))\n",
    "input,label =batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2048, 21])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device id cuda:0\n",
      "base lr: 1.92e-03\n",
      "actual lr: 4.69e-05\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 5\n",
      "Forward pass time: 0.152402 seconds\n",
      "Forward and backward pass time: 0.361839 seconds\n",
      "FLOPs: 51.097429 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "args.model = 'RmGPT'\n",
    "#释放显存\n",
    "torch.cuda.empty_cache()\n",
    "Exp = Exp_All_Task_SUP(args)\n",
    "model  = Exp.model\n",
    "# 示例调用（需要根据实际情况定义模型、输入、损失函数和优化器）\n",
    "criterion = Exp._select_criterion(Exp.task_data_config_list)[0]  # 定义损失函数\n",
    "optimizer = Exp._select_optimizer()  # 定义优化器\n",
    "evaluate_model_efficiency(model, input, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device id cuda:0\n",
      "base lr: 1.92e-03\n",
      "actual lr: 4.69e-05\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 5\n",
      "Forward pass time: 0.092807 seconds\n",
      "Forward and backward pass time: 0.377280 seconds\n",
      "FLOPs: 51.106890 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "args.model = 'RmGPT_woAtten'\n",
    "#释放显存\n",
    "torch.cuda.empty_cache()\n",
    "Exp = Exp_All_Task_SUP(args)\n",
    "model  = Exp.model\n",
    "optimizer = Exp._select_optimizer()  # 定义优化器\n",
    "evaluate_model_efficiency(model, input, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device id cuda:0\n",
      "base lr: 1.92e-03\n",
      "actual lr: 4.69e-05\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 5\n",
      "Forward pass time: 10.131988 seconds\n",
      "Forward and backward pass time: 30.976710 seconds\n",
      "FLOPs: 3813.998093 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "args.model = 'RmGPT_woPatch'\n",
    "#释放显存\n",
    "torch.cuda.empty_cache()\n",
    "Exp = Exp_All_Task_SUP(args)\n",
    "model  = Exp.model\n",
    "# 示例调用（需要根据实际情况定义模型、输入、损失函数和优化器）\n",
    "optimizer = Exp._select_optimizer()  # 定义优化器\n",
    "evaluate_model_efficiency(model, input, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device id cuda:0\n",
      "base lr: 3.00e-04\n",
      "actual lr: 3.00e-04\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 32\n",
      "Forward pass time: 0.049698 seconds\n",
      "Forward and backward pass time: 0.220226 seconds\n",
      "FLOPs: 654.047087 GFLOPs\n",
      "GPU Memory Usage: 60.33 GB / 192.00 GB\n"
     ]
    }
   ],
   "source": [
    "args.model = 'RmGPT'\n",
    "#释放显存\n",
    "torch.cuda.empty_cache()\n",
    "Exp = Exp_All_Task_SUP(args)\n",
    "model  = Exp.model\n",
    "optimizer = Exp._select_optimizer()  # 定义优化器\n",
    "evaluate_model_efficiency(model, input, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable: prompt_tokens.SMU\n",
      "trainable: prompt_tokens.PHM_Challenge2024\n",
      "trainable: cls_tokens.PHM_SMU\n",
      "trainable: cls_tokens.PHM_Challenge2024\n",
      "trainable: category_tokens.PHM_SMU\n",
      "trainable: category_tokens.PHM_Challenge2024\n",
      "trainable: cls_token_head.weight\n",
      "trainable: cls_token_head.bias\n",
      "base lr: 3.00e-04\n",
      "actual lr: 3.00e-04\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 32\n",
      "Forward pass time: 0.034220 seconds\n",
      "Forward and backward pass time: 0.067653 seconds\n",
      "FLOPs: 654.047087 GFLOPs\n",
      "GPU Memory Usage: 57.17 GB / 192.00 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for name, param in model.named_parameters():\n",
    "        #仅仅训练网络中保存的任务参数\n",
    "        if  'prompt_token' in name or 'mask_prompt' in name or 'cls_prompt' in name or 'mask_token' in name  or 'cls_token' in name or 'category_token' in name :\n",
    "            param.requires_grad = True\n",
    "            print(\"trainable:\", name)\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "optimizer = Exp._select_optimizer()  # 定义优化器\n",
    "evaluate_model_efficiency(model, input, criterion, optimizer)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable: prompt_tokens.SMU\n",
      "trainable: prompt_tokens.PHM_Challenge2024\n",
      "trainable: cls_tokens.PHM_SMU\n",
      "trainable: cls_tokens.PHM_Challenge2024\n",
      "trainable: category_tokens.PHM_SMU\n",
      "trainable: category_tokens.PHM_Challenge2024\n",
      "trainable: cls_token_head.weight\n",
      "trainable: cls_token_head.bias\n",
      "Total trainable parameters: 3.037696\n",
      "Total non-trainable parameters: 66.048257\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "non_trainable_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # 仅仅训练网络中保存的任务参数\n",
    "    if 'prompt_token' in name or 'mask_prompt' in name or 'cls_prompt' in name or 'mask_token' in name or 'cls_token' in name or 'category_token' in name:\n",
    "        param.requires_grad = True\n",
    "        trainable_params += param.numel()\n",
    "        print(\"trainable:\", name)\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        non_trainable_params += param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters: {trainable_params/1e6}\")\n",
    "print(f\"Total non-trainable parameters: {non_trainable_params/1e6}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
